import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Embedding, GlobalMaxPooling1D
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np
from collections import Counter
from sklearn.model_selection import train_test_split
import pickle
import tensorflow as tf



data = pd.read_csv('../Dataset_cleaned.csv')
data['content'] = data['content'].str.replace('[^ã„±-ã… ã…-ã…£ ê°€-í£ ]','',regex=True)
label_counts = data["label"].value_counts(normalize=True) * 100
invalid_label_rows = data[~data['label'].astype(str).isin(['0', '1'])]  # ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ í•„í„°ë§
#í† í°í™”í•œê±° ê°€ì§€ê³  ã…—ã…‡ê¸°ê¸°
with open('tokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)



data_shuffled = data.sample(frac=1, random_state=777).reset_index(drop=True)

# ğŸ”¹ í…ìŠ¤íŠ¸ì™€ ë¼ë²¨ ë¶„í• 
train_texts, temp_texts, train_labels, temp_labels = train_test_split(
    data_shuffled['content'], data_shuffled['label'], test_size=0.3, random_state=777
)  # 70% train

val_texts, test_texts, val_labels, test_labels = train_test_split(
    temp_texts, temp_labels, test_size=2/3, random_state=777
)  # 10% val, 20% test

# ğŸ”¹ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜ (ê° ë°ì´í„°ì…‹ ë³„ë¡œ ì²˜ë¦¬)
train_sequences = tokenizer.texts_to_sequences(train_texts)
val_sequences = tokenizer.texts_to_sequences(val_texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)

# ğŸ”¹ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì • (ì˜ˆ: 100)
maxlen = 100

# ğŸ”¹ íŒ¨ë”© ì ìš©
x_train = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=maxlen)
x_val = tf.keras.preprocessing.sequence.pad_sequences(val_sequences, maxlen=maxlen)
x_test = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=maxlen)

# ğŸ”¹ ë¼ë²¨ì„ NumPy ë°°ì—´ë¡œ ë³€í™˜
y_train = np.array(train_labels)
y_val = np.array(val_labels)
y_test = np.array(test_labels)
print(f"y_train shape: {y_train.shape}")
print(f"y_train[:5]: {y_train[:5]}")


print(f"Train shape: {x_train.shape}, Validation shape: {x_val.shape}, Test shape: {x_test.shape}")
print(f"Train Labels shape: {y_train.shape}, Validation Labels shape: {y_val.shape}, Test Labels shape: {y_test.shape}")
print(tokenizer.word_index)  # ê²°ê³¼ í™•ì¸



# âœ… ë°ì´í„° ë³€í™˜ (float32ë¡œ ë³€í™˜)
x_train = np.array(x_train, dtype=np.int32)
x_val = np.array(x_val, dtype=np.int32)
y_train = np.array(y_train, dtype=np.float32).reshape(-1, 1)
y_val = np.array(y_val, dtype=np.float32).reshape(-1, 1)

# âœ… ëª¨ë¸ ì •ì˜ (ì¶œë ¥ ì°¨ì› ìˆ˜ì •)
# ì˜¬ë°”ë¥¸ vocab_size ì„¤ì •
vocab_size = max(x_train.max(), x_val.max()) + 1

embedding_dim = 16
maxlen = 100


model = tf.keras.models.Sequential([
    Embedding(vocab_size, embedding_dim, input_length=maxlen),
    Bidirectional(LSTM(100, return_sequences=True)),
    GlobalMaxPooling1D(),  # âœ… LSTMì˜ ì¶œë ¥ ì°¨ì› ì¤„ì´ê¸°
    Dense(128, activation='relu'),
    Dense(1, activation='sigmoid')  # âœ… ì´ì§„ ë¶„ë¥˜ì´ë¯€ë¡œ sigmoid ì‚¬ìš©
])

# âœ… ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
checkpoint_path = 'best_performed_model.weights.h5'
checkpoint = tf.keras.callbacks.ModelCheckpoint(
    checkpoint_path, 
    save_weights_only=True,
    save_best_only=True,
    monitor='val_loss',
    verbose=1
)

# âœ… ëª¨ë¸ ì»´íŒŒì¼
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# âœ… ëª¨ë¸ í•™ìŠµ
history = model.fit(
    x_train, y_train, 
    validation_data=(x_val, y_val),
    callbacks=[checkpoint],
    epochs=20, 
    verbose=2
)
model.save("best_performed_model.h5")  # âœ… ëª¨ë¸ ì „ì²´ ì €ì¥
m_pred = model.predict(x_test)  # ğŸ”¹ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ê°’
pred = (m_pred > 0.5).astype(int)  # ğŸ”¹ í™•ë¥ ì„ 0 ë˜ëŠ” 1ë¡œ ë³€í™˜
true = y_test  # ì‹¤ì œ ë¼ë²¨

# âœ… ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
print("âœ… ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸")
print(f"True labels: {true[:10].flatten()}")  # ğŸ”¹ ì‹¤ì œ ë¼ë²¨ ì¶œë ¥
print(f"Predicted labels: {pred[:10].flatten()}")  # ğŸ”¹ ì˜ˆì¸¡ëœ ë¼ë²¨ ì¶œë ¥